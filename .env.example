# Environment Variables for GovernmentReporter
# Copy this file to .env and fill in your actual values
# NEVER commit .env file to version control

# ============================================================================
# Required API Keys
# ============================================================================

# OpenAI API Key (required for embeddings and metadata extraction)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here

# CourtListener API Token (required for SCOTUS ingestion)
# Free registration at: https://www.courtlistener.com/api/
COURT_LISTENER_API_TOKEN=your-courtlistener-token-here


# ============================================================================
# Qdrant Vector Database Configuration
# ============================================================================

# Qdrant connection settings
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Qdrant database path (for local storage)
QDRANT_DB_PATH=./data/qdrant/qdrant_db

# Optional: Qdrant API key (for cloud/remote instances)
# QDRANT_API_KEY=your-qdrant-api-key


# ============================================================================
# MCP Server Configuration
# ============================================================================

# Server identification
MCP_SERVER_NAME=GovernmentReporter MCP Server
MCP_SERVER_VERSION=0.1.0

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
MCP_LOG_LEVEL=INFO

# Search defaults
MCP_DEFAULT_SEARCH_LIMIT=10
MCP_MAX_SEARCH_LIMIT=50

# Enable result caching (true/false)
MCP_ENABLE_CACHE=true


# ============================================================================
# Ingestion Configuration
# ============================================================================

# Progress tracking database paths
SCOTUS_PROGRESS_DB=./data/progress/scotus_ingestion.db
EO_PROGRESS_DB=./data/progress/executive_orders_ingestion.db

# Batch sizes for ingestion
SCOTUS_BATCH_SIZE=50
EO_BATCH_SIZE=50

# Date ranges for Docker Compose ingestion
# These are used by docker-compose ingestion services
# Format: YYYY-MM-DD
SCOTUS_START_DATE=2024-01-01
SCOTUS_END_DATE=2024-12-31
EO_START_DATE=2024-01-01
EO_END_DATE=2024-12-31


# ============================================================================
# Chunking Configuration (Advanced)
# ============================================================================

# SCOTUS chunking parameters
RAG_SCOTUS_MIN_TOKENS=500
RAG_SCOTUS_TARGET_TOKENS=600
RAG_SCOTUS_MAX_TOKENS=800
RAG_SCOTUS_OVERLAP_RATIO=0.15

# Executive Order chunking parameters
RAG_EO_MIN_TOKENS=240
RAG_EO_TARGET_TOKENS=340
RAG_EO_MAX_TOKENS=400
RAG_EO_OVERLAP_RATIO=0.10


# ============================================================================
# Embedding Configuration
# ============================================================================

# OpenAI embedding model
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSIONS=1536


# ============================================================================
# LLM Extraction Configuration
# ============================================================================

# Model for metadata extraction
LLM_EXTRACTION_MODEL=gpt-5-mini
LLM_EXTRACTION_TEMPERATURE=0.1
LLM_EXTRACTION_MAX_TOKENS=1000


# ============================================================================
# Logging Configuration
# ============================================================================

# Log directory
LOG_DIR=./logs

# Log level for different components
LOG_LEVEL_ROOT=INFO
LOG_LEVEL_APIS=INFO
LOG_LEVEL_DATABASE=INFO
LOG_LEVEL_INGESTION=INFO
LOG_LEVEL_PROCESSORS=INFO
LOG_LEVEL_SERVER=DEBUG


# ============================================================================
# Development Settings
# ============================================================================

# Enable debug mode (more verbose logging, detailed errors)
DEBUG=false

# Enable dry run mode (no actual storage)
DRY_RUN=false
